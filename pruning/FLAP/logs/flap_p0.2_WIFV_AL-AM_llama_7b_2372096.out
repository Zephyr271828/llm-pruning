torch 1.10.1
transformers 4.28.0
accelerate 0.18.0
# of gpus:  1
loading llm model /n/fs/vision-mix/yx1168/model_ckpts/llama-7b-hf
<class 'models.hf_llama.modeling_llama.LlamaForCausalLM'>
use device  cuda:0
pruning starts
loading calibdation data
dataset loading complete
******************************
layer 0 sparsity 1.000040
layer 1 sparsity 1.000040
layer 2 sparsity 1.000040
layer 3 sparsity 1.000040
layer 4 sparsity 1.000040
layer 5 sparsity 1.000040
layer 6 sparsity 1.000040
layer 7 sparsity 1.000040
layer 8 sparsity 1.000040
layer 9 sparsity 1.000040
layer 10 sparsity 1.000040
layer 11 sparsity 1.000040
layer 12 sparsity 1.000040
layer 13 sparsity 1.000040
layer 14 sparsity 1.000040
layer 15 sparsity 1.000040
layer 16 sparsity 1.000040
layer 17 sparsity 1.000040
layer 18 sparsity 1.000040
layer 19 sparsity 1.000040
layer 20 sparsity 1.000040
layer 21 sparsity 1.000040
layer 22 sparsity 1.000040
layer 23 sparsity 1.000040
layer 24 sparsity 1.000040
layer 25 sparsity 1.000040
layer 26 sparsity 1.000040
layer 27 sparsity 1.000040
layer 28 sparsity 1.000040
layer 29 sparsity 1.000040
layer 30 sparsity 1.000040
layer 31 sparsity 1.000040
sparsity sanity check 1.0000
model parameter 6.74B
******************************
evaluating on wikitext2
nsamples 2667
sample 0
sample 50
sample 100
sample 150
sample 200
sample 250
sample 300
sample 350
sample 400
sample 450
sample 500
sample 550
sample 600
sample 650
sample 700
sample 750
sample 800
sample 850
sample 900
sample 950
sample 1000
sample 1050
sample 1100
sample 1150
sample 1200
sample 1250
sample 1300
sample 1350
sample 1400
sample 1450
sample 1500
sample 1550
sample 1600
sample 1650
sample 1700
sample 1750
sample 1800
sample 1850
sample 1900
sample 1950
sample 2000
sample 2050
sample 2100
sample 2150
sample 2200
sample 2250
sample 2300
sample 2350
sample 2400
sample 2450
sample 2500
sample 2550
sample 2600
sample 2650
ppl on wikitext 15.05293083190918
model and tokenizer loaded from llm_weights/flap_p0.2_WIFV_AL-AM_llama_7b
