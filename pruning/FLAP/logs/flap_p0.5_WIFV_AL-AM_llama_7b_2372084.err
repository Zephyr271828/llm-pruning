/n/fs/vision-mix/yx1168/conda_envs/flap/lib/python3.9/site-packages/torch/torch_version.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
/n/fs/vision-mix/yx1168/conda_envs/flap/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/n/fs/vision-mix/yx1168/conda_envs/flap/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.26s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.88s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.09s/it]
Processing layers:   0%|          | 0/32 [00:00<?, ?it/s]Processing layers:   3%|▎         | 1/32 [00:01<01:00,  1.94s/it]Processing layers:   6%|▋         | 2/32 [00:03<00:48,  1.61s/it]Processing layers:   9%|▉         | 3/32 [00:04<00:43,  1.50s/it]Processing layers:  12%|█▎        | 4/32 [00:06<00:40,  1.45s/it]Processing layers:  16%|█▌        | 5/32 [00:07<00:38,  1.42s/it]Processing layers:  19%|█▉        | 6/32 [00:08<00:36,  1.40s/it]Processing layers:  22%|██▏       | 7/32 [00:10<00:34,  1.39s/it]Processing layers:  25%|██▌       | 8/32 [00:11<00:33,  1.39s/it]Processing layers:  28%|██▊       | 9/32 [00:12<00:31,  1.38s/it]Processing layers:  31%|███▏      | 10/32 [00:14<00:30,  1.38s/it]Processing layers:  34%|███▍      | 11/32 [00:15<00:28,  1.37s/it]Processing layers:  38%|███▊      | 12/32 [00:17<00:27,  1.37s/it]Processing layers:  41%|████      | 13/32 [00:18<00:26,  1.37s/it]Processing layers:  44%|████▍     | 14/32 [00:19<00:24,  1.37s/it]Processing layers:  47%|████▋     | 15/32 [00:21<00:23,  1.37s/it]Processing layers:  50%|█████     | 16/32 [00:22<00:21,  1.37s/it]Processing layers:  53%|█████▎    | 17/32 [00:23<00:20,  1.37s/it]Processing layers:  56%|█████▋    | 18/32 [00:25<00:19,  1.37s/it]Processing layers:  59%|█████▉    | 19/32 [00:26<00:17,  1.37s/it]Processing layers:  62%|██████▎   | 20/32 [00:27<00:16,  1.37s/it]Processing layers:  66%|██████▌   | 21/32 [00:29<00:15,  1.37s/it]Processing layers:  69%|██████▉   | 22/32 [00:30<00:13,  1.37s/it]Processing layers:  72%|███████▏  | 23/32 [00:32<00:12,  1.37s/it]Processing layers:  75%|███████▌  | 24/32 [00:33<00:10,  1.37s/it]Processing layers:  78%|███████▊  | 25/32 [00:34<00:09,  1.37s/it]Processing layers:  81%|████████▏ | 26/32 [00:36<00:08,  1.37s/it]Processing layers:  84%|████████▍ | 27/32 [00:37<00:06,  1.37s/it]Processing layers:  88%|████████▊ | 28/32 [00:38<00:05,  1.37s/it]Processing layers:  91%|█████████ | 29/32 [00:40<00:04,  1.37s/it]Processing layers:  94%|█████████▍| 30/32 [00:41<00:02,  1.37s/it]Processing layers:  97%|█████████▋| 31/32 [00:42<00:01,  1.37s/it]Processing layers: 100%|██████████| 32/32 [00:44<00:00,  1.37s/it]Processing layers: 100%|██████████| 32/32 [00:44<00:00,  1.39s/it]
/n/fs/vision-mix/yx1168/conda_envs/flap/lib/python3.9/site-packages/torch/torch_version.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
/n/fs/vision-mix/yx1168/conda_envs/flap/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/n/fs/vision-mix/yx1168/conda_envs/flap/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:25<00:25, 25.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:34<00:00, 15.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:34<00:00, 17.29s/it]
Some weights of the model checkpoint at llm_weights/flap_p0.5_WIFV_AL-AM_llama_7b were not used when initializing LlamaForCausalLM: ['model.layers.1.self_attn.o_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.25.self_attn.o_proj.bias']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/n/fs/vision-mix/yx1168/pruning/fms-wanda/wanda/lib/eval.py", line 192, in <module>
    results = eval_zero_shot(args.model_name, model, tokenizer,
  File "/n/fs/vision-mix/yx1168/pruning/fms-wanda/wanda/lib/eval.py", line 141, in eval_zero_shot
    from lm_eval import tasks, evaluator
  File "/n/fs/vision-mix/yx1168/pruning/fms-wanda/wanda/lm-evaluation-harness/lm_eval/tasks/__init__.py", line 5, in <module>
    import lm_eval.base
  File "/n/fs/vision-mix/yx1168/pruning/fms-wanda/wanda/lm-evaluation-harness/lm_eval/base.py", line 17, in <module>
    from lm_eval import utils
  File "/n/fs/vision-mix/yx1168/pruning/fms-wanda/wanda/lm-evaluation-harness/lm_eval/utils.py", line 14, in <module>
    from omegaconf import OmegaConf
ModuleNotFoundError: No module named 'omegaconf'
