/n/fs/vision-mix/yx1168/conda_envs/flap/lib/python3.9/site-packages/torch/torch_version.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
/n/fs/vision-mix/yx1168/conda_envs/flap/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/n/fs/vision-mix/yx1168/conda_envs/flap/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:17<00:17, 17.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 11.19s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.18s/it]
/n/fs/vision-mix/yx1168/conda_envs/flap/lib/python3.9/site-packages/torch/torch_version.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
/n/fs/vision-mix/yx1168/conda_envs/flap/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/n/fs/vision-mix/yx1168/conda_envs/flap/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:25<00:25, 25.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:34<00:00, 15.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:34<00:00, 17.05s/it]
Some weights of the model checkpoint at llm_weights/wanda_sp_p0.5_N/A_N/A_llama_7b were not used when initializing LlamaForCausalLM: ['model.layers.23.self_attn.o_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.7.mlp.down_proj.bias']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/n/fs/vision-mix/yx1168/pruning/fms-wanda/wanda/lib/eval.py", line 192, in <module>
    results = eval_zero_shot(args.model_name, model, tokenizer,
  File "/n/fs/vision-mix/yx1168/pruning/fms-wanda/wanda/lib/eval.py", line 141, in eval_zero_shot
    from lm_eval import tasks, evaluator
  File "/n/fs/vision-mix/yx1168/pruning/fms-wanda/wanda/lm-evaluation-harness/lm_eval/tasks/__init__.py", line 5, in <module>
    import lm_eval.base
  File "/n/fs/vision-mix/yx1168/pruning/fms-wanda/wanda/lm-evaluation-harness/lm_eval/base.py", line 17, in <module>
    from lm_eval import utils
  File "/n/fs/vision-mix/yx1168/pruning/fms-wanda/wanda/lm-evaluation-harness/lm_eval/utils.py", line 14, in <module>
    from omegaconf import OmegaConf
ModuleNotFoundError: No module named 'omegaconf'
